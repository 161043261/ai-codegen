# Server
PORT=8123
API_PREFIX=api

# Database
DB_HOST=localhost
DB_PORT=3306
DB_USERNAME=root
DB_PASSWORD=pass
DB_DATABASE=ai_codegen

# Redis
REDIS_ENABLED=false
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_TTL=3600

# Session
SESSION_STORE_TYPE=memory
SESSION_SECRET=ai-codegen
SESSION_MAX_AGE=2592000000

# AI Models Provider: ollama or cloud
CHAT_MODEL_PROVIDER=ollama

# ====== Cloud Provider (CHAT_MODEL_PROVIDER=cloud) ======

# AI Models - Streaming Chat Model (DeepSeek Chat)
STREAMING_CHAT_MODEL_BASE_URL=https://api.deepseek.com
STREAMING_CHAT_MODEL_API_KEY=https://platform.deepseek.com/api_keys
STREAMING_CHAT_MODEL_NAME=deepseek-chat
STREAMING_CHAT_MODEL_MAX_TOKENS=8192

# AI Models - Reasoning Streaming Chat Model (DeepSeek Reasoner)
REASONING_STREAMING_CHAT_MODEL_BASE_URL=https://api.deepseek.com
REASONING_STREAMING_CHAT_MODEL_API_KEY=https://platform.deepseek.com/api_keys
REASONING_STREAMING_CHAT_MODEL_NAME=deepseek-reasoner
REASONING_STREAMING_CHAT_MODEL_MAX_TOKENS=8192
REASONING_STREAMING_CHAT_MODEL_TEMPERATURE=0.1

# AI Models - Route Chat Model (Qwen)
ROUTE_CHAT_MODEL_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
ROUTE_CHAT_MODEL_API_KEY="https://bailian.console.aliyun.com/cn-beijing/?tab=model#/api-key"
ROUTE_CHAT_MODEL_NAME=qwen-turbo
ROUTE_CHAT_MODEL_MAX_TOKENS=100

# ====== Ollama Provider (CHAT_MODEL_PROVIDER=ollama) ======

OLLAMA_BASE_URL=http://localhost:11434
# Streaming Chat Model (Vanilla HTML or multiple files)
OLLAMA_STREAMING_CHAT_MODEL=qwen3.5:cloud
OLLAMA_STREAMING_CHAT_MAX_TOKENS=8192
# Reasoning Streaming Chat Model (Vite Project)
OLLAMA_REASONING_CHAT_MODEL=qwen3
OLLAMA_REASONING_CHAT_MAX_TOKENS=8192
OLLAMA_REASONING_CHAT_TEMPERATURE=0.1
# Route Chat Model
OLLAMA_ROUTE_CHAT_MODEL=qwen2.5
OLLAMA_ROUTE_CHAT_MAX_TOKENS=100

# Storage
STORAGE_LOCAL_PATH=./static
STORAGE_BASE_URL=http://localhost:8123/api/static

# Deploy
CODEGEN_DEPLOY_HOST=http://localhost:8123/api
